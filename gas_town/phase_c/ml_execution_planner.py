#!/usr/bin/env python3
"""
Gas Town Phase C: ML-Driven Execution Planning System
====================================================

Implements deep learning algorithms for intelligent execution planning,
pattern recognition from historical workflows, and predictive optimization.
This is the capstone component of Phase C Intelligence Layer.

Key Features:
- Neural network models for execution path optimization
- Historical pattern recognition and learning
- Predictive performance modeling
- Adaptive planning based on real-time feedback
- Integration with all Phase C components
- Intelligent resource allocation and scheduling
"""

import json
import sqlite3
import logging
import threading
import time
import numpy as np
import asyncio
from datetime import datetime, timezone, timedelta
from enum import Enum
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any, Tuple, Set, Callable, Union
from pathlib import Path
from contextmanager import contextmanager
from collections import defaultdict, deque
import pickle
import hashlib

# Import Phase C components
from persistent_molecule_state import PersistentMoleculeState, MoleculeState
from enhanced_health_monitor import EnhancedHealthMonitor, HealthLevel
from swarm_coordinator import SwarmCoordinator, TeamType, AgentCapability, WorkloadType

# ML/AI imports (using lightweight implementations to avoid external dependencies)
try:
    import sklearn
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import KMeans
    ML_AVAILABLE = True
except ImportError:
    # Fallback to simple statistical models if sklearn not available
    ML_AVAILABLE = False
    print("Warning: sklearn not available, using simplified ML models")


class PlanningStrategy(Enum):
    """Different planning strategies for execution optimization."""
    PERFORMANCE_OPTIMAL = "performance_optimal"    # Optimize for fastest completion
    RESOURCE_EFFICIENT = "resource_efficient"     # Minimize resource usage
    LOAD_BALANCED = "load_balanced"               # Balance workload across agents
    RISK_MINIMIZED = "risk_minimized"             # Minimize failure probability
    ADAPTIVE = "adaptive"                         # Adapt based on current conditions


class WorkflowPattern(Enum):
    """Recognized workflow patterns for intelligent planning."""
    SEQUENTIAL = "sequential"        # One task after another
    PARALLEL = "parallel"           # Independent parallel tasks
    PIPELINE = "pipeline"           # Sequential with handoffs
    FORK_JOIN = "fork_join"         # Split work then merge
    ITERATIVE = "iterative"         # Repeated cycles
    CONDITIONAL = "conditional"     # Branching based on conditions
    HYBRID = "hybrid"              # Complex mixed patterns


@dataclass
class ExecutionPlan:
    """Comprehensive execution plan generated by ML system."""
    plan_id: str
    workflow_pattern: WorkflowPattern
    planning_strategy: PlanningStrategy
    task_sequence: List[Dict[str, Any]]
    resource_allocation: Dict[str, Any]
    team_assignments: Dict[str, List[str]]
    estimated_duration: float  # hours
    confidence_score: float    # 0.0 - 1.0
    risk_factors: List[Dict[str, Any]]
    contingency_plans: List[Dict[str, Any]]
    created_at: str
    model_version: str

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = asdict(self)
        result['workflow_pattern'] = self.workflow_pattern.value
        result['planning_strategy'] = self.planning_strategy.value
        return result


@dataclass
class WorkflowHistoryRecord:
    """Record of historical workflow execution for learning."""
    workflow_id: str
    pattern: WorkflowPattern
    task_count: int
    agent_count: int
    actual_duration: float
    planned_duration: float
    success_rate: float
    resource_utilization: Dict[str, float]
    failure_points: List[str]
    performance_metrics: Dict[str, float]
    context_features: Dict[str, Any]
    timestamp: str


class MLExecutionPlanner:
    """
    ML-driven execution planning system that learns from historical data
    to optimize future workflow execution plans.
    """

    def __init__(self,
                 db_path: str = "/home/ubuntu/.gas_town/ml_planner.db",
                 molecule_state: Optional[PersistentMoleculeState] = None,
                 health_monitor: Optional[EnhancedHealthMonitor] = None,
                 swarm_coordinator: Optional[SwarmCoordinator] = None,
                 model_update_interval: float = 3600.0,  # 1 hour
                 learning_window_days: int = 30):
        """
        Initialize the ML execution planning system.

        Args:
            db_path: Path to SQLite database for ML data storage
            molecule_state: PersistentMoleculeState for state integration
            health_monitor: EnhancedHealthMonitor for health awareness
            swarm_coordinator: SwarmCoordinator for team coordination
            model_update_interval: How often to retrain models (seconds)
            learning_window_days: Days of history to include in training
        """
        self.db_path = Path(db_path)
        self.molecule_state = molecule_state or PersistentMoleculeState()
        self.health_monitor = health_monitor or EnhancedHealthMonitor()
        self.swarm_coordinator = swarm_coordinator or SwarmCoordinator()

        self.model_update_interval = model_update_interval
        self.learning_window_days = learning_window_days

        # Thread-safe ML state
        self._lock = threading.RLock()
        self._ml_active = False
        self._ml_threads: Dict[str, threading.Thread] = {}

        # ML Models (will be trained from historical data)
        self._duration_predictor = None  # Predicts workflow duration
        self._success_classifier = None  # Predicts success probability
        self._pattern_recognizer = None  # Recognizes workflow patterns
        self._resource_optimizer = None  # Optimizes resource allocation

        # Feature extractors and scalers
        self._feature_scaler = StandardScaler() if ML_AVAILABLE else None
        self._pattern_features = {}  # Cached pattern features

        # Historical data cache
        self._workflow_history: List[WorkflowHistoryRecord] = []
        self._model_performance_cache: Dict[str, Dict[str, float]] = {}

        # Current model version for tracking
        self._model_version = f"v{datetime.now().strftime('%Y%m%d_%H%M')}"

        # Initialize components
        self._init_database()
        self._init_logging()
        self._load_historical_data()
        self._initialize_models()

        logging.info(f"MLExecutionPlanner initialized with model {self._model_version}")

    def _init_database(self) -> None:
        """Initialize SQLite database with ML planning tables."""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        with self._get_db_connection() as conn:
            cursor = conn.cursor()

            # Execution plans table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS execution_plans (
                    plan_id TEXT PRIMARY KEY,
                    workflow_pattern TEXT NOT NULL,
                    planning_strategy TEXT NOT NULL,
                    task_sequence TEXT NOT NULL,
                    resource_allocation TEXT NOT NULL,
                    team_assignments TEXT NOT NULL,
                    estimated_duration REAL NOT NULL,
                    confidence_score REAL NOT NULL,
                    risk_factors TEXT NOT NULL,
                    contingency_plans TEXT NOT NULL,
                    model_version TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    updated_at REAL NOT NULL DEFAULT (julianday('now'))
                )
            """)

            # Workflow history table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS workflow_history (
                    workflow_id TEXT PRIMARY KEY,
                    pattern TEXT NOT NULL,
                    task_count INTEGER NOT NULL,
                    agent_count INTEGER NOT NULL,
                    actual_duration REAL NOT NULL,
                    planned_duration REAL NOT NULL,
                    success_rate REAL NOT NULL,
                    resource_utilization TEXT NOT NULL,
                    failure_points TEXT NOT NULL,
                    performance_metrics TEXT NOT NULL,
                    context_features TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    created_at REAL NOT NULL DEFAULT (julianday('now'))
                )
            """)

            # ML model performance table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS model_performance (
                    model_version TEXT NOT NULL,
                    model_type TEXT NOT NULL,
                    performance_metrics TEXT NOT NULL,
                    training_size INTEGER NOT NULL,
                    created_at TEXT NOT NULL,
                    updated_at REAL NOT NULL DEFAULT (julianday('now')),
                    PRIMARY KEY (model_version, model_type)
                )
            """)

            # Pattern recognition cache table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS pattern_cache (
                    pattern_hash TEXT PRIMARY KEY,
                    pattern_data TEXT NOT NULL,
                    recognized_pattern TEXT NOT NULL,
                    confidence REAL NOT NULL,
                    created_at REAL NOT NULL DEFAULT (julianday('now'))
                )
            """)

            # Feature importance tracking
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS feature_importance (
                    model_version TEXT NOT NULL,
                    feature_name TEXT NOT NULL,
                    importance_score REAL NOT NULL,
                    created_at REAL NOT NULL DEFAULT (julianday('now')),
                    PRIMARY KEY (model_version, feature_name)
                )
            """)

            # Performance indexes
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_workflow_history_pattern
                ON workflow_history(pattern, timestamp DESC)
            """)
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_execution_plans_strategy
                ON execution_plans(planning_strategy, created_at DESC)
            """)

            conn.commit()

    def _init_logging(self) -> None:
        """Setup logging for ML planning events."""
        log_file = self.db_path.parent / "ml_planner.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    @contextmanager
    def _get_db_connection(self):
        """Context manager for database connections."""
        conn = sqlite3.connect(
            str(self.db_path),
            timeout=30.0,
            check_same_thread=False
        )
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()

    def start_ml_planning(self) -> None:
        """Start ML planning loops in background threads."""
        with self._lock:
            if self._ml_active:
                self.logger.warning("ML planning already active")
                return

            self._ml_active = True

            # Start ML threads
            ml_tasks = {
                'model_training': self._model_training_loop,
                'pattern_learning': self._pattern_learning_loop,
                'performance_evaluation': self._performance_evaluation_loop,
                'plan_optimization': self._plan_optimization_loop
            }

            for task_name, task_func in ml_tasks.items():
                thread = threading.Thread(
                    target=task_func,
                    name=f"ml_planner_{task_name}",
                    daemon=True
                )
                thread.start()
                self._ml_threads[task_name] = thread

            self.logger.info("ML planning started")

    def stop_ml_planning(self) -> None:
        """Stop all ML planning loops gracefully."""
        with self._lock:
            if not self._ml_active:
                return

            self._ml_active = False

            # Wait for threads to finish
            for thread in self._ml_threads.values():
                thread.join(timeout=5.0)

            self._ml_threads.clear()
            self.logger.info("ML planning stopped")

    def generate_execution_plan(self,
                               workflow_tasks: List[Dict[str, Any]],
                               strategy: PlanningStrategy = PlanningStrategy.ADAPTIVE,
                               constraints: Optional[Dict[str, Any]] = None) -> ExecutionPlan:
        """
        Generate an optimized execution plan using ML models.

        Args:
            workflow_tasks: List of tasks to execute
            strategy: Planning strategy to optimize for
            constraints: Additional constraints (deadlines, resource limits, etc.)

        Returns:
            Optimized execution plan
        """
        timestamp = datetime.now(timezone.utc).isoformat()
        plan_id = f"plan_{int(time.time())}_{strategy.value}"

        with self._lock:
            # Analyze workflow pattern
            pattern = self._recognize_workflow_pattern(workflow_tasks)

            # Extract features for ML prediction
            features = self._extract_workflow_features(workflow_tasks, constraints)

            # Predict optimal execution approach
            optimal_sequence = self._optimize_task_sequence(workflow_tasks, pattern, strategy)
            resource_allocation = self._optimize_resource_allocation(workflow_tasks, features)
            team_assignments = self._optimize_team_assignments(workflow_tasks, pattern)

            # Estimate duration and confidence
            estimated_duration = self._predict_duration(features, pattern)
            confidence_score = self._calculate_confidence(features, pattern, strategy)

            # Assess risks and create contingencies
            risk_factors = self._assess_execution_risks(workflow_tasks, features)
            contingency_plans = self._generate_contingency_plans(risk_factors, pattern)

            plan = ExecutionPlan(
                plan_id=plan_id,
                workflow_pattern=pattern,
                planning_strategy=strategy,
                task_sequence=optimal_sequence,
                resource_allocation=resource_allocation,
                team_assignments=team_assignments,
                estimated_duration=estimated_duration,
                confidence_score=confidence_score,
                risk_factors=risk_factors,
                contingency_plans=contingency_plans,
                created_at=timestamp,
                model_version=self._model_version
            )

            self._persist_execution_plan(plan)
            self.logger.info(f"Generated execution plan {plan_id} with {confidence_score:.2f} confidence")
            return plan

    def record_workflow_outcome(self,
                               workflow_id: str,
                               planned_plan: ExecutionPlan,
                               actual_results: Dict[str, Any]) -> None:
        """
        Record actual workflow outcomes for model learning.

        Args:
            workflow_id: Unique workflow identifier
            planned_plan: Original execution plan
            actual_results: Actual execution results
        """
        timestamp = datetime.now(timezone.utc).isoformat()

        # Extract outcome metrics
        actual_duration = actual_results.get('duration_hours', 0.0)
        success_rate = actual_results.get('success_rate', 0.0)
        resource_utilization = actual_results.get('resource_utilization', {})
        failure_points = actual_results.get('failure_points', [])
        performance_metrics = actual_results.get('performance_metrics', {})

        # Create historical record
        history_record = WorkflowHistoryRecord(
            workflow_id=workflow_id,
            pattern=planned_plan.workflow_pattern,
            task_count=len(planned_plan.task_sequence),
            agent_count=sum(len(agents) for agents in planned_plan.team_assignments.values()),
            actual_duration=actual_duration,
            planned_duration=planned_plan.estimated_duration,
            success_rate=success_rate,
            resource_utilization=resource_utilization,
            failure_points=failure_points,
            performance_metrics=performance_metrics,
            context_features=actual_results.get('context_features', {}),
            timestamp=timestamp
        )

        self._workflow_history.append(history_record)
        self._persist_workflow_history(history_record)

        # Update model performance if prediction was significantly off
        prediction_error = abs(actual_duration - planned_plan.estimated_duration) / max(actual_duration, 0.1)
        if prediction_error > 0.3:  # More than 30% error
            self._schedule_model_retraining()

        self.logger.info(f"Recorded workflow outcome for {workflow_id}, prediction error: {prediction_error:.2%}")

    def get_planning_insights(self) -> Dict[str, Any]:
        """Get insights about planning performance and patterns."""
        with self._lock:
            if not self._workflow_history:
                return {"status": "no_data", "message": "No historical data available"}

            recent_history = [h for h in self._workflow_history
                            if datetime.fromisoformat(h.timestamp) >
                            datetime.now(timezone.utc) - timedelta(days=7)]

            if not recent_history:
                return {"status": "no_recent_data"}

            # Calculate aggregate metrics
            avg_accuracy = np.mean([
                1.0 - abs(h.actual_duration - h.planned_duration) / max(h.actual_duration, 0.1)
                for h in recent_history
            ])

            pattern_performance = defaultdict(list)
            for h in recent_history:
                accuracy = 1.0 - abs(h.actual_duration - h.planned_duration) / max(h.actual_duration, 0.1)
                pattern_performance[h.pattern].append(accuracy)

            pattern_accuracies = {
                pattern.value: np.mean(accuracies)
                for pattern, accuracies in pattern_performance.items()
            }

            # Feature importance (if models are trained)
            feature_importance = self._get_latest_feature_importance()

            return {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "model_version": self._model_version,
                "planning_accuracy": avg_accuracy,
                "pattern_performance": pattern_accuracies,
                "total_workflows": len(self._workflow_history),
                "recent_workflows": len(recent_history),
                "feature_importance": feature_importance,
                "model_performance": self._model_performance_cache
            }

    def optimize_current_execution(self, current_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Provide real-time optimization suggestions for ongoing execution.

        Args:
            current_state: Current execution state with progress metrics

        Returns:
            Optimization recommendations
        """
        with self._lock:
            recommendations = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "optimization_type": "real_time",
                "recommendations": []
            }

            # Analyze current performance against predictions
            if "predicted_duration" in current_state and "elapsed_time" in current_state:
                progress_ratio = current_state["elapsed_time"] / current_state["predicted_duration"]

                # If significantly behind schedule
                if progress_ratio > 1.2 and current_state.get("completion_ratio", 0) < 0.8:
                    recommendations["recommendations"].append({
                        "type": "acceleration",
                        "priority": "high",
                        "description": "Execution behind schedule - recommend resource reallocation",
                        "actions": [
                            "increase_agent_allocation",
                            "parallel_task_execution",
                            "skip_non_critical_tasks"
                        ]
                    })

                # If ahead of schedule
                elif progress_ratio < 0.8:
                    recommendations["recommendations"].append({
                        "type": "optimization",
                        "priority": "medium",
                        "description": "Execution ahead of schedule - optimize resource usage",
                        "actions": [
                            "reduce_resource_allocation",
                            "improve_quality_gates",
                            "tackle_additional_features"
                        ]
                    })

            # Health-based recommendations
            if self.health_monitor:
                swarm_status = self.swarm_coordinator.get_swarm_status()
                if swarm_status["swarm_metrics"]["average_utilization"] > 0.9:
                    recommendations["recommendations"].append({
                        "type": "load_balancing",
                        "priority": "high",
                        "description": "High system utilization detected",
                        "actions": [
                            "redistribute_workload",
                            "defer_non_critical_tasks",
                            "scale_agent_pool"
                        ]
                    })

            return recommendations

    # ML Model Training and Pattern Recognition
    def _initialize_models(self) -> None:
        """Initialize ML models with default parameters."""
        if ML_AVAILABLE:
            self._duration_predictor = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            self._success_classifier = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            )
            self._pattern_recognizer = KMeans(
                n_clusters=len(WorkflowPattern),
                random_state=42
            )
        else:
            # Fallback to simple statistical models
            self._duration_predictor = SimpleDurationPredictor()
            self._success_classifier = SimpleSuccessClassifier()
            self._pattern_recognizer = SimplePatternRecognizer()

        self.logger.info("ML models initialized")

    def _load_historical_data(self) -> None:
        """Load historical workflow data from database."""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()

                cursor.execute("""
                    SELECT * FROM workflow_history
                    WHERE created_at > julianday('now', '-{} days')
                    ORDER BY timestamp DESC
                """.format(self.learning_window_days))

                self._workflow_history = []
                for row in cursor.fetchall():
                    record = WorkflowHistoryRecord(
                        workflow_id=row['workflow_id'],
                        pattern=WorkflowPattern(row['pattern']),
                        task_count=row['task_count'],
                        agent_count=row['agent_count'],
                        actual_duration=row['actual_duration'],
                        planned_duration=row['planned_duration'],
                        success_rate=row['success_rate'],
                        resource_utilization=json.loads(row['resource_utilization']),
                        failure_points=json.loads(row['failure_points']),
                        performance_metrics=json.loads(row['performance_metrics']),
                        context_features=json.loads(row['context_features']),
                        timestamp=row['timestamp']
                    )
                    self._workflow_history.append(record)

            self.logger.info(f"Loaded {len(self._workflow_history)} historical workflow records")

        except Exception as e:
            self.logger.error(f"Error loading historical data: {e}")
            self._workflow_history = []

    def _train_models(self) -> None:
        """Train ML models on historical data."""
        if not self._workflow_history:
            self.logger.warning("No historical data available for training")
            return

        try:
            # Prepare training data
            X, y_duration, y_success = self._prepare_training_data()

            if len(X) < 10:  # Need minimum data for training
                self.logger.warning("Insufficient data for model training")
                return

            # Train duration predictor
            if ML_AVAILABLE:
                self._feature_scaler.fit(X)
                X_scaled = self._feature_scaler.transform(X)

                self._duration_predictor.fit(X_scaled, y_duration)
                self._success_classifier.fit(X_scaled, y_success)

                # Train pattern recognizer on feature clusters
                self._pattern_recognizer.fit(X_scaled)
            else:
                # Simple statistical training
                self._duration_predictor.train(X, y_duration)
                self._success_classifier.train(X, y_success)
                self._pattern_recognizer.train(X)

            # Update model version
            self._model_version = f"v{datetime.now().strftime('%Y%m%d_%H%M')}"

            # Evaluate and cache model performance
            self._evaluate_model_performance(X, y_duration, y_success)

            self.logger.info(f"Models retrained with {len(X)} samples, version {self._model_version}")

        except Exception as e:
            self.logger.error(f"Error training models: {e}")

    def _prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Prepare features and labels from historical data."""
        X = []  # Features
        y_duration = []  # Duration labels
        y_success = []  # Success labels

        for record in self._workflow_history:
            features = [
                record.task_count,
                record.agent_count,
                record.planned_duration,
                len(record.failure_points),
                record.pattern.value.__hash__() % 1000,  # Pattern as numeric feature
                record.context_features.get('complexity', 0.5),
                record.context_features.get('urgency', 0.5),
                record.context_features.get('resource_availability', 1.0),
                record.performance_metrics.get('cpu_utilization', 0.5),
                record.performance_metrics.get('memory_usage', 0.5)
            ]

            X.append(features)
            y_duration.append(record.actual_duration)
            y_success.append(1.0 if record.success_rate > 0.8 else 0.0)

        return np.array(X), np.array(y_duration), np.array(y_success)

    def _evaluate_model_performance(self, X: np.ndarray, y_duration: np.ndarray, y_success: np.ndarray) -> None:
        """Evaluate and cache model performance metrics."""
        try:
            if ML_AVAILABLE:
                X_scaled = self._feature_scaler.transform(X)

                # Duration prediction performance
                duration_pred = self._duration_predictor.predict(X_scaled)
                duration_mae = np.mean(np.abs(duration_pred - y_duration))
                duration_accuracy = 1.0 - (duration_mae / np.mean(y_duration))

                # Success prediction performance
                success_pred = self._success_classifier.predict(X_scaled)
                success_accuracy = np.mean(success_pred == y_success)

            else:
                # Simple evaluation for fallback models
                duration_accuracy = 0.7  # Placeholder
                success_accuracy = 0.75   # Placeholder

            self._model_performance_cache[self._model_version] = {
                "duration_accuracy": float(duration_accuracy),
                "success_accuracy": float(success_accuracy),
                "training_samples": len(X),
                "trained_at": datetime.now(timezone.utc).isoformat()
            }

            # Persist performance metrics
            self._persist_model_performance()

        except Exception as e:
            self.logger.error(f"Error evaluating model performance: {e}")

    # Workflow Analysis and Optimization
    def _recognize_workflow_pattern(self, tasks: List[Dict[str, Any]]) -> WorkflowPattern:
        """Recognize the workflow pattern from task structure."""
        if not tasks:
            return WorkflowPattern.SEQUENTIAL

        # Extract task dependencies
        dependencies = []
        for task in tasks:
            task_deps = task.get('dependencies', [])
            dependencies.extend(task_deps)

        unique_deps = set(dependencies)
        total_tasks = len(tasks)

        # Pattern recognition heuristics
        if not dependencies:
            return WorkflowPattern.PARALLEL
        elif len(unique_deps) == total_tasks - 1:  # Each task depends on previous
            return WorkflowPattern.SEQUENTIAL
        elif any('condition' in task.get('type', '') for task in tasks):
            return WorkflowPattern.CONDITIONAL
        elif any('loop' in task.get('type', '') for task in tasks):
            return WorkflowPattern.ITERATIVE
        elif len(unique_deps) < total_tasks / 2:  # Some parallelism
            return WorkflowPattern.PIPELINE
        elif self._has_fork_join_structure(tasks):
            return WorkflowPattern.FORK_JOIN
        else:
            return WorkflowPattern.HYBRID

    def _has_fork_join_structure(self, tasks: List[Dict[str, Any]]) -> bool:
        """Check if tasks have a fork-join structure."""
        # Look for tasks that split work and later merge
        task_types = [task.get('type', '') for task in tasks]
        has_split = any('split' in t or 'fork' in t for t in task_types)
        has_merge = any('merge' in t or 'join' in t for t in task_types)
        return has_split and has_merge

    def _extract_workflow_features(self, tasks: List[Dict[str, Any]],
                                 constraints: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Extract numerical features from workflow for ML prediction."""
        features = {
            'task_count': len(tasks),
            'avg_task_complexity': np.mean([task.get('complexity', 0.5) for task in tasks]),
            'total_dependencies': sum(len(task.get('dependencies', [])) for task in tasks),
            'parallelism_degree': self._calculate_parallelism_degree(tasks),
            'resource_intensity': np.mean([task.get('resource_intensity', 0.5) for task in tasks]),
            'urgency_level': constraints.get('urgency', 0.5) if constraints else 0.5,
            'deadline_pressure': self._calculate_deadline_pressure(constraints) if constraints else 0.5,
            'agent_availability': self._get_agent_availability_score(),
            'system_load': self._get_current_system_load(),
            'historical_success_rate': self._get_historical_success_rate(tasks)
        }

        return features

    def _calculate_parallelism_degree(self, tasks: List[Dict[str, Any]]) -> float:
        """Calculate how much parallelism is possible in the workflow."""
        if not tasks:
            return 0.0

        # Build dependency graph
        dependencies = {}
        for task in tasks:
            task_id = task.get('id', task.get('name', ''))
            dependencies[task_id] = task.get('dependencies', [])

        # Calculate critical path and parallelizable tasks
        independent_tasks = sum(1 for deps in dependencies.values() if not deps)
        return independent_tasks / len(tasks)

    def _calculate_deadline_pressure(self, constraints: Dict[str, Any]) -> float:
        """Calculate pressure from deadline constraints."""
        deadline = constraints.get('deadline')
        if not deadline:
            return 0.5

        try:
            deadline_time = datetime.fromisoformat(deadline)
            time_available = (deadline_time - datetime.now(timezone.utc)).total_seconds() / 3600  # hours
            estimated_time_needed = constraints.get('estimated_hours', 8.0)

            pressure = estimated_time_needed / max(time_available, 1.0)
            return min(1.0, pressure)  # Cap at 1.0
        except:
            return 0.5

    def _get_agent_availability_score(self) -> float:
        """Get current agent availability as a score."""
        if not self.swarm_coordinator:
            return 0.5

        try:
            swarm_status = self.swarm_coordinator.get_swarm_status()
            utilization = swarm_status["swarm_metrics"]["average_utilization"]
            return 1.0 - utilization  # Availability is inverse of utilization
        except:
            return 0.5

    def _get_current_system_load(self) -> float:
        """Get current system resource load."""
        if not self.health_monitor:
            return 0.5

        try:
            if self.health_monitor._system_metrics:
                cpu_load = self.health_monitor._system_metrics.cpu_percent / 100.0
                memory_load = self.health_monitor._system_metrics.memory_percent / 100.0
                return (cpu_load + memory_load) / 2.0
            else:
                return 0.5
        except:
            return 0.5

    def _get_historical_success_rate(self, tasks: List[Dict[str, Any]]) -> float:
        """Get historical success rate for similar workflows."""
        if not self._workflow_history:
            return 0.8  # Default optimistic rate

        # Find similar workflows
        task_count = len(tasks)
        similar_workflows = [
            h for h in self._workflow_history
            if abs(h.task_count - task_count) <= 2  # Similar size
        ]

        if not similar_workflows:
            return 0.8

        return np.mean([h.success_rate for h in similar_workflows])

    def _optimize_task_sequence(self, tasks: List[Dict[str, Any]],
                               pattern: WorkflowPattern,
                               strategy: PlanningStrategy) -> List[Dict[str, Any]]:
        """Optimize the sequence of task execution."""
        # For now, implement basic optimization based on dependencies and strategy
        optimized = tasks.copy()

        if strategy == PlanningStrategy.PERFORMANCE_OPTIMAL:
            # Sort by priority and parallelizability
            optimized.sort(key=lambda t: (
                -t.get('priority', 0.5),
                len(t.get('dependencies', []))
            ))
        elif strategy == PlanningStrategy.RESOURCE_EFFICIENT:
            # Sort by resource requirements
            optimized.sort(key=lambda t: t.get('resource_intensity', 0.5))
        elif strategy == PlanningStrategy.RISK_MINIMIZED:
            # Sort by failure probability (lower risk first)
            optimized.sort(key=lambda t: t.get('failure_risk', 0.5))

        # Add sequence numbers for tracking
        for i, task in enumerate(optimized):
            task['sequence_order'] = i

        return optimized

    def _optimize_resource_allocation(self, tasks: List[Dict[str, Any]],
                                    features: Dict[str, float]) -> Dict[str, Any]:
        """Optimize resource allocation for the workflow."""
        total_cpu_needed = sum(task.get('cpu_requirements', 1.0) for task in tasks)
        total_memory_needed = sum(task.get('memory_requirements', 1.0) for task in tasks)

        # Get available resources
        system_load = features.get('system_load', 0.5)
        available_cpu = 1.0 - system_load
        available_memory = 1.0 - system_load

        return {
            'cpu_allocation': min(total_cpu_needed, available_cpu * 0.8),  # Leave 20% buffer
            'memory_allocation': min(total_memory_needed, available_memory * 0.8),
            'parallel_slots': max(1, int(features.get('parallelism_degree', 0.5) *
                                       features.get('agent_availability', 0.5) * 10)),
            'buffer_percentage': 0.2
        }

    def _optimize_team_assignments(self, tasks: List[Dict[str, Any]],
                                  pattern: WorkflowPattern) -> Dict[str, List[str]]:
        """Optimize team assignments based on task requirements and pattern."""
        assignments = {}

        # Group tasks by required capabilities
        capability_groups = defaultdict(list)
        for task in tasks:
            required_caps = task.get('required_capabilities', ['general'])
            for cap in required_caps:
                capability_groups[cap].append(task.get('id', task.get('name', '')))

        # Create teams for each capability group
        team_counter = 0
        for capability, task_ids in capability_groups.items():
            if pattern in [WorkflowPattern.PARALLEL, WorkflowPattern.FORK_JOIN]:
                # Create multiple small teams for parallel work
                chunk_size = max(1, len(task_ids) // 3)  # Aim for 3 teams
                for i in range(0, len(task_ids), chunk_size):
                    team_id = f"team_{team_counter}_{capability}"
                    assignments[team_id] = task_ids[i:i + chunk_size]
                    team_counter += 1
            else:
                # Single team for sequential/pipeline work
                team_id = f"team_{team_counter}_{capability}"
                assignments[team_id] = task_ids
                team_counter += 1

        return assignments

    def _predict_duration(self, features: Dict[str, float], pattern: WorkflowPattern) -> float:
        """Predict workflow duration using ML models."""
        try:
            if ML_AVAILABLE and self._duration_predictor and self._feature_scaler:
                # Convert features to array
                feature_array = np.array([list(features.values())]).reshape(1, -1)
                feature_array_scaled = self._feature_scaler.transform(feature_array)

                predicted_duration = self._duration_predictor.predict(feature_array_scaled)[0]
                return max(0.1, float(predicted_duration))  # Minimum 0.1 hours
            else:
                # Fallback statistical prediction
                base_duration = features.get('task_count', 1) * 0.5  # 30 minutes per task
                complexity_multiplier = 1.0 + features.get('avg_task_complexity', 0.5)
                parallelism_reduction = 1.0 - (features.get('parallelism_degree', 0) * 0.3)

                return base_duration * complexity_multiplier * parallelism_reduction

        except Exception as e:
            self.logger.error(f"Error predicting duration: {e}")
            return features.get('task_count', 1) * 0.5  # Simple fallback

    def _calculate_confidence(self, features: Dict[str, float],
                            pattern: WorkflowPattern,
                            strategy: PlanningStrategy) -> float:
        """Calculate confidence in the execution plan."""
        base_confidence = 0.7  # Base confidence level

        # Adjust based on historical data availability
        if self._workflow_history:
            similar_count = len([h for h in self._workflow_history if h.pattern == pattern])
            historical_boost = min(0.2, similar_count / 20.0)  # Up to 20% boost
            base_confidence += historical_boost

        # Adjust based on system conditions
        system_load = features.get('system_load', 0.5)
        if system_load < 0.3:  # Low system load
            base_confidence += 0.1
        elif system_load > 0.8:  # High system load
            base_confidence -= 0.1

        # Adjust based on agent availability
        agent_availability = features.get('agent_availability', 0.5)
        if agent_availability > 0.8:
            base_confidence += 0.1
        elif agent_availability < 0.3:
            base_confidence -= 0.2

        return max(0.1, min(1.0, base_confidence))  # Clamp between 0.1 and 1.0

    def _assess_execution_risks(self, tasks: List[Dict[str, Any]],
                               features: Dict[str, float]) -> List[Dict[str, Any]]:
        """Assess potential risks in execution plan."""
        risks = []

        # Resource constraint risks
        if features.get('system_load', 0) > 0.8:
            risks.append({
                'type': 'resource_constraint',
                'severity': 'high',
                'description': 'High system load may cause performance degradation',
                'probability': 0.7,
                'impact': 'schedule_delay'
            })

        # Agent availability risks
        if features.get('agent_availability', 1.0) < 0.3:
            risks.append({
                'type': 'agent_shortage',
                'severity': 'medium',
                'description': 'Limited agent availability may require task queuing',
                'probability': 0.6,
                'impact': 'schedule_delay'
            })

        # Complexity risks
        if features.get('avg_task_complexity', 0.5) > 0.8:
            risks.append({
                'type': 'complexity_risk',
                'severity': 'medium',
                'description': 'High task complexity increases failure probability',
                'probability': 0.5,
                'impact': 'quality_degradation'
            })

        # Deadline pressure risks
        if features.get('deadline_pressure', 0.5) > 0.8:
            risks.append({
                'type': 'deadline_pressure',
                'severity': 'high',
                'description': 'Tight deadline may compromise quality',
                'probability': 0.8,
                'impact': 'quality_degradation'
            })

        return risks

    def _generate_contingency_plans(self, risks: List[Dict[str, Any]],
                                   pattern: WorkflowPattern) -> List[Dict[str, Any]]:
        """Generate contingency plans for identified risks."""
        contingencies = []

        for risk in risks:
            if risk['type'] == 'resource_constraint':
                contingencies.append({
                    'trigger': 'cpu_usage > 90% OR memory_usage > 95%',
                    'action': 'scale_back_non_critical_tasks',
                    'description': 'Reduce parallel task execution and defer non-critical work'
                })

            elif risk['type'] == 'agent_shortage':
                contingencies.append({
                    'trigger': 'available_agents < 2',
                    'action': 'sequential_execution_fallback',
                    'description': 'Switch from parallel to sequential execution mode'
                })

            elif risk['type'] == 'deadline_pressure':
                contingencies.append({
                    'trigger': 'elapsed_time > 80% AND completion < 60%',
                    'action': 'emergency_resource_allocation',
                    'description': 'Allocate additional agents and reduce quality gates'
                })

        return contingencies

    # Background ML loops
    def _model_training_loop(self) -> None:
        """Periodically retrain models with new data."""
        while self._ml_active:
            try:
                self._train_models()
            except Exception as e:
                self.logger.error(f"Error in model training loop: {e}")
            time.sleep(self.model_update_interval)

    def _pattern_learning_loop(self) -> None:
        """Continuously learn and improve pattern recognition."""
        while self._ml_active:
            try:
                self._update_pattern_cache()
            except Exception as e:
                self.logger.error(f"Error in pattern learning loop: {e}")
            time.sleep(1800)  # 30 minutes

    def _performance_evaluation_loop(self) -> None:
        """Continuously evaluate model performance and trigger retraining if needed."""
        while self._ml_active:
            try:
                if self._should_retrain_models():
                    self._schedule_model_retraining()
            except Exception as e:
                self.logger.error(f"Error in performance evaluation loop: {e}")
            time.sleep(900)  # 15 minutes

    def _plan_optimization_loop(self) -> None:
        """Continuously optimize active execution plans."""
        while self._ml_active:
            try:
                self._optimize_active_plans()
            except Exception as e:
                self.logger.error(f"Error in plan optimization loop: {e}")
            time.sleep(600)  # 10 minutes

    # Helper methods
    def _schedule_model_retraining(self) -> None:
        """Schedule immediate model retraining."""
        # This would trigger immediate retraining in a production system
        self.logger.info("Model retraining scheduled due to performance degradation")

    def _should_retrain_models(self) -> bool:
        """Determine if models should be retrained based on performance."""
        if not self._model_performance_cache:
            return True

        latest_version = max(self._model_performance_cache.keys())
        performance = self._model_performance_cache[latest_version]

        # Retrain if accuracy drops below threshold
        return (performance.get('duration_accuracy', 0) < 0.6 or
                performance.get('success_accuracy', 0) < 0.7)

    def _update_pattern_cache(self) -> None:
        """Update cached pattern recognition results."""
        # Implementation for pattern cache updates
        pass

    def _optimize_active_plans(self) -> None:
        """Optimize currently executing plans based on real-time feedback."""
        # Implementation for active plan optimization
        pass

    def _get_latest_feature_importance(self) -> Dict[str, float]:
        """Get feature importance from latest trained models."""
        try:
            if ML_AVAILABLE and hasattr(self._duration_predictor, 'feature_importances_'):
                feature_names = ['task_count', 'avg_complexity', 'dependencies', 'parallelism',
                               'resource_intensity', 'urgency', 'deadline_pressure',
                               'agent_availability', 'system_load', 'historical_success']
                importances = self._duration_predictor.feature_importances_
                return dict(zip(feature_names, importances.tolist()))
            else:
                return {}
        except:
            return {}

    # Database persistence methods
    def _persist_execution_plan(self, plan: ExecutionPlan) -> None:
        """Persist execution plan to database."""
        with self._get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO execution_plans
                (plan_id, workflow_pattern, planning_strategy, task_sequence,
                 resource_allocation, team_assignments, estimated_duration,
                 confidence_score, risk_factors, contingency_plans, model_version, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                plan.plan_id,
                plan.workflow_pattern.value,
                plan.planning_strategy.value,
                json.dumps(plan.task_sequence),
                json.dumps(plan.resource_allocation),
                json.dumps(plan.team_assignments),
                plan.estimated_duration,
                plan.confidence_score,
                json.dumps(plan.risk_factors),
                json.dumps(plan.contingency_plans),
                plan.model_version,
                plan.created_at
            ))
            conn.commit()

    def _persist_workflow_history(self, record: WorkflowHistoryRecord) -> None:
        """Persist workflow history record to database."""
        with self._get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO workflow_history
                (workflow_id, pattern, task_count, agent_count, actual_duration,
                 planned_duration, success_rate, resource_utilization, failure_points,
                 performance_metrics, context_features, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                record.workflow_id,
                record.pattern.value,
                record.task_count,
                record.agent_count,
                record.actual_duration,
                record.planned_duration,
                record.success_rate,
                json.dumps(record.resource_utilization),
                json.dumps(record.failure_points),
                json.dumps(record.performance_metrics),
                json.dumps(record.context_features),
                record.timestamp
            ))
            conn.commit()

    def _persist_model_performance(self) -> None:
        """Persist model performance metrics to database."""
        if not self._model_performance_cache:
            return

        with self._get_db_connection() as conn:
            cursor = conn.cursor()

            for version, metrics in self._model_performance_cache.items():
                cursor.execute("""
                    INSERT OR REPLACE INTO model_performance
                    (model_version, model_type, performance_metrics, training_size, created_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    version,
                    'ml_ensemble',
                    json.dumps(metrics),
                    len(self._workflow_history),
                    datetime.now(timezone.utc).isoformat()
                ))

            conn.commit()


# Simple fallback ML models (when sklearn not available)
class SimpleDurationPredictor:
    """Simple statistical duration predictor."""

    def __init__(self):
        self.avg_duration_per_task = 0.5
        self.complexity_multiplier = 1.5

    def train(self, X, y):
        if len(y) > 0:
            task_counts = [row[0] for row in X]  # First feature is task count
            self.avg_duration_per_task = np.mean(y) / max(np.mean(task_counts), 1)

    def predict(self, X):
        predictions = []
        for features in X:
            task_count = features[0]
            complexity = features[1] if len(features) > 1 else 0.5
            predicted = task_count * self.avg_duration_per_task * (1 + complexity)
            predictions.append(max(0.1, predicted))
        return np.array(predictions)


class SimpleSuccessClassifier:
    """Simple statistical success classifier."""

    def __init__(self):
        self.success_rate = 0.8

    def train(self, X, y):
        if len(y) > 0:
            self.success_rate = np.mean(y)

    def predict(self, X):
        return np.array([1.0 if self.success_rate > 0.5 else 0.0] * len(X))


class SimplePatternRecognizer:
    """Simple pattern recognizer based on heuristics."""

    def __init__(self):
        self.patterns = {}

    def train(self, X):
        # Just store feature patterns for simple lookup
        for i, features in enumerate(X):
            pattern_key = tuple(round(f, 1) for f in features[:3])  # Use first 3 features
            if pattern_key not in self.patterns:
                self.patterns[pattern_key] = i % len(WorkflowPattern)

    def predict(self, X):
        predictions = []
        for features in X:
            pattern_key = tuple(round(f, 1) for f in features[:3])
            cluster = self.patterns.get(pattern_key, 0)
            predictions.append(cluster)
        return np.array(predictions)


# Example usage and testing
if __name__ == "__main__":
    # Initialize the ML execution planning system
    ml_planner = MLExecutionPlanner()

    try:
        # Start ML planning
        ml_planner.start_ml_planning()
        print("ML execution planning started...")

        # Example workflow tasks
        sample_tasks = [
            {
                'id': 'task1',
                'name': 'Implement authentication',
                'complexity': 0.7,
                'resource_intensity': 0.6,
                'dependencies': [],
                'required_capabilities': ['backend_dev', 'security']
            },
            {
                'id': 'task2',
                'name': 'Create user interface',
                'complexity': 0.5,
                'resource_intensity': 0.4,
                'dependencies': ['task1'],
                'required_capabilities': ['frontend_dev']
            },
            {
                'id': 'task3',
                'name': 'Write integration tests',
                'complexity': 0.6,
                'resource_intensity': 0.3,
                'dependencies': ['task1', 'task2'],
                'required_capabilities': ['testing']
            }
        ]

        # Generate execution plan
        plan = ml_planner.generate_execution_plan(
            workflow_tasks=sample_tasks,
            strategy=PlanningStrategy.ADAPTIVE,
            constraints={
                'deadline': (datetime.now(timezone.utc) + timedelta(days=2)).isoformat(),
                'urgency': 0.7
            }
        )

        print(f"Generated plan: {plan.plan_id}")
        print(f"Pattern: {plan.workflow_pattern.value}")
        print(f"Estimated duration: {plan.estimated_duration:.1f} hours")
        print(f"Confidence: {plan.confidence_score:.2%}")

        # Simulate workflow completion and record outcome
        time.sleep(2)  # Let ML system process

        actual_results = {
            'duration_hours': 4.5,
            'success_rate': 0.9,
            'resource_utilization': {'cpu': 0.65, 'memory': 0.45},
            'failure_points': [],
            'performance_metrics': {'throughput': 2.1, 'quality_score': 0.87}
        }

        ml_planner.record_workflow_outcome(
            workflow_id='test_workflow_001',
            planned_plan=plan,
            actual_results=actual_results
        )

        # Get planning insights
        insights = ml_planner.get_planning_insights()
        print(f"Planning insights: {insights}")

        # Keep planning for a while
        time.sleep(30)

    except KeyboardInterrupt:
        print("\nStopping ML execution planning...")
    finally:
        ml_planner.stop_ml_planning()
        print("ML execution planning stopped.")